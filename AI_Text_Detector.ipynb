{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFirhIXmeINr5dUSKug5FW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buhlerja/APS360_Team32/blob/main/AI_Text_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import torchtext.vocab as vocab\n",
        "\n",
        "glove_embeddings = vocab.GloVe(name='6B', dim=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlpJoaxRQ1lH",
        "outputId": "e2e047ca-89a9-4d9e-f98a-f2cbf6fded2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:26<00:00, 14952.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "vAAmeLwrS2H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conjunctions = ['and', 'but', 'or', 'nor', 'for', 'yet', 'so', 'although',\n",
        "                               'because', 'since', 'while', 'after', 'before', 'when', 'if',\n",
        "                               'unless', 'until', 'whether']\n",
        "\n",
        "def remove_conjunctions(text, conjunctions):\n",
        "  '''\n",
        "  string, list -> string\n",
        "  -\n",
        "  function takes in an essay (string) and list of conjunctions (list) and outputs the input essay with\n",
        "  instances of the conjunctions in the list removed\n",
        "  '''\n",
        "  words = text.split()\n",
        "  words = [word for word in words if word.lower() not in conjunctions]\n",
        "  return ' '.join(words)\n",
        "\n",
        "import string\n",
        "# Function to remove punctuation from a string\n",
        "def remove_punctuation(text):\n",
        "    return ''.join(char for char in text if char not in string.punctuation)\n",
        "\n"
      ],
      "metadata": {
        "id": "xTEclFEtS05a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition"
      ],
      "metadata": {
        "id": "4ewdE_jzQoCQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEenZTozOx-a"
      },
      "outputs": [],
      "source": [
        "# Model code\n",
        "class AI_Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(AI_Classifier, self).__init__()\n",
        "        self.name = \"AI_Classifier\"\n",
        "        self.emb = nn.Embedding.from_pretrained(glove_embeddings.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size) # Output size should = 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Look up embeddings\n",
        "        # Mask out-of-range indices\n",
        "        mask = (x >= 0) & (x < self.emb.num_embeddings)\n",
        "        x = torch.where(mask, x, torch.zeros_like(x))\n",
        "        #print(x.shape)\n",
        "        x = self.emb(x)\n",
        "        #print(x.shape)\n",
        "        # Set the initial hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        # Forward propagate\n",
        "        out, _ = self.rnn(x, h0) # Expects input 3D tensor of (batch_size, seq_length, input_size)\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # Apply softmax to get 0/1\n",
        "        return F.log_softmax(out, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Model"
      ],
      "metadata": {
        "id": "o--Y1J2RRa5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "modelPath = '/content/gdrive/My Drive/APS360/Project/model.pth'\n",
        "\n",
        "# Define the model\n",
        "input_size = 100  # size of word embeddings\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 2  # binary classification (human or AI generated)\n",
        "seq_length = 20  # length of input sequence\n",
        "\n",
        "# Initialize the LSTM classifier\n",
        "Test_Model = AI_Classifier(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "state_dict = torch.load(modelPath)\n",
        "\n",
        "# Apply the state dictionary to the model\n",
        "Test_Model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGd-HeZ0Rdea",
        "outputId": "fb8dd9f0-b4f3-49fd-fabb-d2809e767b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Model"
      ],
      "metadata": {
        "id": "-iAvj8bASXw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################ INPUT GOES HERE ################\n",
        "\n",
        "essay = '\"The Chrysalids\" by John Wyndham immerses readers into a dystopian world fraught with societal paranoia and genetic discrimination. Set in the aftermath of a devastating nuclear holocaust, the story unfolds in the insulated community of Waknuk, where the rigid principles of purity and conformity dominate every aspect of life. The protagonist, David Strorm, grapples with the burden of his telepathic abilities, deemed blasphemous in a society that deifies genetic uniformity. As David navigates the oppressive atmosphere of Waknuk, he forms a clandestine bond with other telepathic individuals, including his cousin Rosalind. Together, they embark on a perilous journey to evade persecution and seek sanctuary in the elusive haven of Sealand. Along the way, they confront the harrowing reality of their worlds relentless pursuit of genetic perfection, as well as the moral ambiguity of their own actions in the face of adversity. Through Wyndhams masterful storytelling, \"The Chrysalids\" delves deep into themes of identity, prejudice, and the innate human desire for acceptance and belonging. As readers follow Davids quest for freedom and self-discovery, they are compelled to reflect on the implications of unchecked societal dogma and the enduring power of empathy and resilience in the face of adversity. With its thought-provoking narrative and richly drawn characters, \"The Chrysalids\" stands as a timeless testament to the enduring struggle for individuality and the human spirits capacity to transcend the confines of prejudice and fear.'\n",
        "#################################################\n",
        "\n",
        "# Build a dictionary to hold the vocabulary and each word's GloVe Index\n",
        "word_to_glove_index = {}\n",
        "words = essay.lower().split()  # Tokenize and convert to lowercase\n",
        "for word in words:\n",
        "    # Check if the word is not punctuation, is in GloVe embeddings, and not already in the vocabulary\n",
        "    if word in glove_embeddings.stoi and word not in word_to_glove_index:\n",
        "        word_to_glove_index[word] = glove_embeddings.stoi[word]\n",
        "\n",
        "# Step 4: Convert essays to sequences of corresponding GloVe indices\n",
        "essay_as_indices = [word_to_glove_index.get(word, -1) for word in words]  # Get GloVe index for each word\n",
        "indexed_essay = essay_as_indices\n",
        "\n",
        "# Step 6: Convert to PyTorch tensor\n",
        "essays_tensor = torch.tensor(indexed_essay, dtype=torch.long)\n",
        "essay_tensor = essays_tensor.reshape(1, len(essays_tensor))\n",
        "\n",
        "result = Test_Model(essay_tensor)\n",
        "probabilities = result\n",
        "# Convert log probabilities to probabilities\n",
        "probabilities = F.softmax(probabilities, dim=1)\n",
        "result = torch.argmax(result, dim=1)\n",
        "\n",
        "if result.item() == 0:\n",
        "  print(\"Human-Generated\")\n",
        "  print(\"Certainty: \", probabilities[0,0].item())\n",
        "elif result.item() == 1:\n",
        "  print(\"AI-Generated\")\n",
        "  print(\"Certainty: \", probabilities[0,1].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sILoeo4aSkoy",
        "outputId": "c2d7b227-a75f-4a16-9c96-2e4ca0fe97e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI-Generated\n",
            "Certainty:  0.9999693632125854\n"
          ]
        }
      ]
    }
  ]
}